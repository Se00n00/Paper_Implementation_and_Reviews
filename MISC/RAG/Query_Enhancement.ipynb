{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5051e309",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ceda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6836a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = os.getenv(\"LLM\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "base_url = os.getenv(\"BASE_URL\")\n",
    "\n",
    "llm = \"openai/gpt-oss-20b\"\n",
    "api_key = \"\"\n",
    "base_url = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = llm,\n",
    "    api_key = api_key,\n",
    "    base_url = base_url,\n",
    "    streaming = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0526a",
   "metadata": {},
   "source": [
    "## Query Enhancement Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81abb101",
   "metadata": {},
   "source": [
    "### Query Re-writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed02508",
   "metadata": {},
   "source": [
    "Makes Query More specific and Detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcc9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rewrite_template = \"\"\"\n",
    "You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "Original query: {original_query}\n",
    "Rewritten query:\n",
    "\"\"\"\n",
    "\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=query_rewrite_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662e74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rewrite_generator(query:str) -> str:\n",
    "    res = model.invoke(\n",
    "        query_rewrite_prompt.invoke({\"original_query\":query})\n",
    "    )\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39f8cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain what constitutes the Solar System, detailing the Sun, the eight planets, dwarf planets, major moons, asteroid belt, Kuiper belt, Oort cloud, and its formation history.\n"
     ]
    }
   ],
   "source": [
    "res = query_rewrite_generator(\"Can you tell me what is the solar system\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075fb6c",
   "metadata": {},
   "source": [
    "### Step-Back Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e078c",
   "metadata": {},
   "source": [
    "Generate More Broader and genralized Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47280f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "StepBack_prompting_template = \"\"\"\n",
    "You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "Don't explain anything just answer the step-back query\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Step-back query:\n",
    "\"\"\"\n",
    "\n",
    "StepBack_prompting_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=StepBack_prompting_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162482df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepback_prompting_generator(query:str) -> str:\n",
    "    res = model.invoke(\n",
    "        StepBack_prompting_prompt.invoke({\"original_query\":query})\n",
    "    )\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1be9ca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a solar system?\n"
     ]
    }
   ],
   "source": [
    "res = stepback_prompting_generator(\"Can you tell me what is the solar system\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b297045",
   "metadata": {},
   "source": [
    "### Sub-Query Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4df11",
   "metadata": {},
   "source": [
    "Decomposes the Queries into Different Sub-queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ccb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subquery_decomposition_template = \"\"\"\n",
    "You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\n",
    "\"\"\"\n",
    "\n",
    "subquery_decomposition_prompt = PromptTemplate(\n",
    "    input_variables = [\"original_query\"],\n",
    "    template = subquery_decomposition_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06b40f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subquery_decomposition_generator(query:str) -> str:\n",
    "    res = model.invoke(\n",
    "        subquery_decomposition_prompt.invoke({\"original_query\":query})\n",
    "    )\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c359597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sub‑queries**\n",
      "\n",
      "1. **What is the definition of the solar system and what are its primary components?**  \n",
      "   (Clarify the concept, the Sun, planets, dwarf planets, moons, asteroid belt, Kuiper belt, Oort cloud, etc.)\n",
      "\n",
      "2. **How are the major celestial bodies in the solar system structured and distributed?**  \n",
      "   (Describe the orbital arrangement, relative sizes, distances, and types of bodies.)\n",
      "\n",
      "3. **What is the formation and evolutionary history of the solar system?**  \n",
      "   (Explain the nebular hypothesis, accretion, planetary differentiation, and current stage of development.)\n",
      "\n",
      "4. **How do dynamic processes (orbital mechanics, gravitational interactions, resonance) govern the behavior and stability of the solar system?**  \n",
      "   (Cover Keplerian orbits, perturbations, resonances, and long‑term stability.)\n"
     ]
    }
   ],
   "source": [
    "res = subquery_decomposition_generator(\"Can you tell me what is the solar system\")\n",
    "print(res)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e3232",
   "metadata": {},
   "source": [
    "### HyDE (Hypothetical Document Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9b8a1",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/NirDiamant/RAG_Techniques/b9617dfe582aae0cd5333a964e6049a4133e04c4/images/HyDe.svg\" height=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5b247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e72bae5",
   "metadata": {},
   "source": [
    "### HyPE (Hypothetical Prompt Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ce149",
   "metadata": {},
   "source": [
    "HyPE precomputes hypothetical questions during the indexing phase. And Retreival done usin Question-Quetion Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ccb85fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hype_template = \"\"\"\n",
    "Analyze the input text and generate essential questions that, when answered,\n",
    "capture the main points of the text. Each question should be one line, without numbering or prefixes.\n",
    "Output formats: \n",
    "questions: list[str]\n",
    "\n",
    "{chunk_text}\n",
    "\n",
    "Questions:\n",
    "\"\"\"\n",
    "\n",
    "hype_prompt = PromptTemplate(\n",
    "    input_variables = [\"chunk_text\"],\n",
    "    template = hype_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a143ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Prompts(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "structured_model = model.with_structured_output(Prompts)\n",
    "def hypothetical_prompt_generator(query:str) -> list[str]:\n",
    "    res = structured_model.invoke(\n",
    "        hype_prompt.invoke({\"chunk_text\":query})\n",
    "    )\n",
    "    return res.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71d193ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_pdf_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf7ccad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_pdf_chunks(\"2506.10943v2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da82a3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How does Test-Time Training modify model weights during inference?',\n",
       " 'What advantage does combining TTT with In-Context Learning provide in few-shot scenarios?',\n",
       " 'How does SEAL implement TTT within its inner-loop optimization?',\n",
       " 'Why is TTT considered more efficient than full-scale training in SEAL?',\n",
       " 'In what way does the method trained on single-example TTT episodes generalize to the continued pretraining regime?',\n",
       " 'What role does reinforcement learning play in improving LLM behavior, particularly in RLHF and verifiable reward settings?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothetical_prompt_generator(res[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22445cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def encode_pdf(chunks: list[str]):\n",
    "    vector_store = None\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        futures = [pool.submit(hypothetical_prompt_generator, c) for c in chunks]\n",
    "\n",
    "        for f in as_completed(futures), len(chunks):\n",
    "            chunk, vectors = f.result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
