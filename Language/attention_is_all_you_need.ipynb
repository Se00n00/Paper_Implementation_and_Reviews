{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884e4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d117017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18143ac1",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "*The Idea is to Augument the token embeddings with position-dependednt pattern of values.* \n",
    "##\n",
    "*If the Pattern is characterisitc for each position, then other layers could learn to incoperate positional information into their transformation.*\n",
    "##\n",
    "***In Other words, if each position has a unique encoding, the model can infer order and distance between tokens.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39118b",
   "metadata": {},
   "source": [
    "### nn.Embedding: Maps the Descrete Input Tokens into Dense Vectors (Embeddings) , a learnable look-up table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d8246",
   "metadata": {},
   "source": [
    "*What if we don't use Embeddings for learning, or transforming into more complex term, instead use things as it , could be use linear with it ?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19de641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of nn.Embedding\n",
    "Num_Unique_Tokens = 1000\n",
    "Size_Of_Vector_to_map = 64\n",
    "enmbedding_layer = nn.Embedding(num_embeddings=Num_Unique_Tokens, embedding_dim=Size_Of_Vector_to_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e0b3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "positional_indices = torch.arange(0, 10).unsqueeze(0)\n",
    "print(positional_indices.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa543577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 5, 7, 9]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_indices[:, 1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafeebcf",
   "metadata": {},
   "source": [
    "*For Positional Informations, max_length defines how long the input sequences can be that the model will see.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d263ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Positional Information using indicies Information, though lacks relative positional information\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_length):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layer_norm = nn.LayerNorm(embed_size, eps=1e-12)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        word_embedding = self.embedding(x)                                                  # Convert unique word tokens to word embeddings\n",
    "        \n",
    "        positional_indices = torch.arange(x.size(-2), device=x.device).unsqueeze(0)         # Creates positional inidices tensor                             Shape: (1, Seqlen)\n",
    "        positional_embeddings = self.position_embedding(positional_indices)                 # Convert positional indicies to positional embeddings          Shape: (1, Seqlen, embed_size)  \n",
    "        \n",
    "        x = word_embedding + positional_embeddings                                          # Adds word embedding to positional embedding\n",
    "        x = self.layer_norm(x)                                                              # Apply layer normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5448099",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usage With Config\n",
    "class Config:\n",
    "    vocab_size = 1000\n",
    "    embed_size = 64\n",
    "    max_length = 100    # Maximum length of the input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bdc09",
   "metadata": {},
   "source": [
    "## Usage ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b377b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding positional Information using sinusoidal function\n",
    "class SinusoidalEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_length, device):\n",
    "        super(SinusoidalEmbeddingLayer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        self.register_buffer(\"positional_embedding\", self._get_positional_encoding(max_length, embed_size, device))\n",
    "        self.layer_norm = nn.LayerNorm(embed_size, eps=1e-12)\n",
    "    \n",
    "    def _get_positional_encoding(self, max_length, embed_size, device):\n",
    "        pe = torch.zeros(max_length, embed_size, device=device)                              # Create a tensor of zeros of size (max_length, embed_size)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)              # Create a tensor of size (max_length, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))    # Create a tensor of exp values of 0 to embed_size/2\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)                                          # Apply sin function to even indices, start=0 , step=2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                                          # Apply cos function to odd indices, start=1, step=2\n",
    "        pe = pe.unsqueeze(0)                                                                  # shape: (1, max_length, embed_size)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        word_embedding = self.embedding(x)                                                  # Convert unique word tokens to word embeddings\n",
    "        \n",
    "        positional_embeddings = self.positional_embedding[:, :x.size(-2), :].to(x.device)   # Get sinosudal indicies information as positional embeddings          Shape: (1, Seqlen, embed_size)\n",
    "        x = word_embedding + positional_embeddings                                          # Adds word embedding to positional embedding\n",
    "        x = self.layer_norm(x)                                                              # Apply layer normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88310358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    vocab_size = 1000\n",
    "    embed_size = 64\n",
    "    max_length = 100    # Maximum length of the input sequence\n",
    "    device = device     # Device to use (CPU or GPU) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3502bd1",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "*Having Attention is finding the similarity between the tokens which adds contextual meaning for input streams of tokens*\n",
    "<Br>\n",
    "*While also having Several Heads allows model to focus on different aspects at once*\n",
    "<Br>\n",
    "*This Works same as convolution filters as one filter looks for face, other for **different features** like car. as same one heads looks for subject-verb interactions, another adjectives and other sentence relations*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8960c5",
   "metadata": {},
   "source": [
    "### Method 1: Implementing Each Head Seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96072647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_size, head_dim):\n",
    "        super(Head, self).__init__()\n",
    "\n",
    "        self.WQ = nn.Linear(embed_size, head_dim)\n",
    "        self.WK = nn.Linear(embed_size, head_dim)\n",
    "        self.WV = nn.Linear(embed_size, head_dim)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=False):\n",
    "        Q = self.WQ(q)                                                                    # Shape: (batch_size, seq_len, head_dim)\n",
    "        K = self.WK(k)                                                                    # Shape: (batch_size, seq_len, head_dim)\n",
    "        V = self.WV(v)                                                                    # Shape: (batch_size, seq_len, head_dim)\n",
    "\n",
    "        # Scaled Dot-Product Attention with masking\n",
    "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(K.size(-1))              # Shape: (batch_size, seq_len, seq_len)\n",
    "        if mask == True:\n",
    "            score = score.masked_fill(mask == 0, float(\"-inf\"))                           # Masking the attention scores\n",
    "        attention_scores = F.softmax(score, dim=-1)                                       # Shape of Attention Scores: (batch_size, seq_len, seq_len)\n",
    "        attention_output = torch.matmul(attention_scores, V)                              # Output Shape: (batch_size, seq_len, head_dim)\n",
    "\n",
    "        return attention_output, attention_scores                                         # Return the attention output and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61a67478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_head_attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Multi_head_attention, self).__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.heads = nn.ModuleList([Head(embed_dim, self.head_dim) for _ in range(num_heads)])\n",
    "        self.WO = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, q, k, v, mask=False):\n",
    "        head_outputs = [h(q, k, v, mask) for h in self.heads]\n",
    "        attention_outputs = [output[0] for output in head_outputs]\n",
    "        attention_scores = [output[1] for output in head_outputs]\n",
    "        \n",
    "        x = torch.cat(attention_outputs, dim=-1)                # Concatenate outputs from all heads\n",
    "        x = self.WO(x)\n",
    "        return x, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22305352",
   "metadata": {},
   "source": [
    "### Method 2: Implementing Head In same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c14b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.WQ = nn.Linear(embed_dim, embed_dim)\n",
    "        self.WK = nn.Linear(embed_dim, embed_dim)\n",
    "        self.WV = nn.Linear(embed_dim, embed_dim)\n",
    "        self.WO = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, q, k, v, masked=False):\n",
    "        batch_size = q.size(0)\n",
    "        q_len, k_len, v_len = q.size(1), k.size(1), v.size(1)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.WQ(q)  # [B, L_q, E]\n",
    "        K = self.WK(k)  # [B, L_k, E]\n",
    "        V = self.WV(v)  # [B, L_v, E]\n",
    "        \n",
    "        # Reshape for multi-head: [B, T, E] → [B, H, L, D]\n",
    "        Q = Q.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, k_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, v_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # Scaled dot-product attention [B, H, L, L]\n",
    "        \n",
    "        # Optional Assertion\n",
    "        assert q_len == k_len, \"Query and Key lengths must be equal for self-attention\"\n",
    "\n",
    "        if masked:\n",
    "            mask = torch.triu(torch.ones(q_len, k_len, device=q.device), diagonal=1).bool() # Create causal mask (for decoder self-attention)\n",
    "            mask = mask.unsqueeze(0).unsqueeze(1)  # [1, 1, L_q, L_k]\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attention_scores = F.softmax(scores, dim=-1)  # Attention Scores[B, H, T_q, T_k]\n",
    "        attention_output = torch.matmul(attention_scores, V)  # [B, H, T_q, D]\n",
    "\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, q_len, self.embed_dim)# Concatenate heads: [B, H, T, D] → [B, T, E]\n",
    "        output = self.WO(attention_output)\n",
    "        \n",
    "        return output, attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a636e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    embed_dim = 64\n",
    "    num_heads = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69ea65",
   "metadata": {},
   "source": [
    "## Feed Forward Layer\n",
    "*It Process Each Embedding Sepeartly insteaad of processing whole as single vector, that's why it is also called **Position-Wise Feed Forward Layer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fa7e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, droupout=0.5):    # ff_dim is usally higher that model_dim\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.droupout = nn.Dropout(droupout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.droupout(x)       # apply dropout to the output of the second linear layer to reduce overfitting\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c26db5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    embed_dim = 512\n",
    "    ff_dim = 2048   # usually 4 times model_dim\n",
    "    dropout = 0.1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63e6dda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (droupout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "feed_forward = FeedForward(Config.model_dim, Config.ff_dim, Config.dropout).to(device)\n",
    "print(feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ec966",
   "metadata": {},
   "source": [
    "## Encoder Layer & Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d00bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though this Implementation is based on the paper \"Attention is All You Need\" by Vaswani et al. (2017) but\n",
    "# for ease of training, we would use Pre-Normallization instead of Post-Normalization\n",
    "class Encoder_Layer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder_Layer, self).__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(config.embed_dim, eps=1e-12)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.embed_dim, eps=1e-12)\n",
    "        self.multi_head_attention = Multi_Head_Attention(config.embed_dim, config.n_heads)\n",
    "        self.feed_forward = FeedForward(config.embed_dim, config.ff_dim, config.dropout)\n",
    "    \n",
    "    def forward(self, x, mask=False):\n",
    "        x = self.layer_norm1(x)                                         # Apply layer1 normalization\n",
    "        attention_output, attention_scores = self.multi_head_attention(x, x, x, mask)\n",
    "        x = x + attention_output                                        # Residual connection\n",
    "        x = self.layer_norm2(x)                                         # Apply layer2 normalization\n",
    "        x = x + self.feed_forward(x)                                    # Residual connection\n",
    "        return x, attention_scores                                      # Return the output and attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0637643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList([Encoder_Layer(config) for _ in range(config.num_encoder_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f1a56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    num_encoder_layers = 2\n",
    "    embed_dim = 512\n",
    "    n_heads = 8\n",
    "    ff_dim = 2048\n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc628a3",
   "metadata": {},
   "source": [
    "## Decoder Layer & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd00edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here also, for ease of training, we would use Pre-Normallization instead of Post-Normalization\n",
    "class Decoder_Layer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Decoder_Layer, self).__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(config.embed_dim, eps=1e-12)\n",
    "        self.layer_norm2 = nn.LayerNorm(config.embed_dim, eps=1e-12)\n",
    "        self.layer_norm3 = nn.LayerNorm(config.embed_dim, eps=1e-12)\n",
    "        self.masked_multi_head_attention = Multi_Head_Attention(config.embed_dim, config.n_heads)\n",
    "        self.multi_head_attention = Multi_Head_Attention(config.embed_dim, config.n_heads)\n",
    "        self.feed_forward = FeedForward(config.embed_dim, config.ff_dim, config.dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        x = self.layer_norm1(x)                                         # Apply layer1 normalization\n",
    "        attention_output, attention_scores = self.masked_multi_head_attention(x, x, x, mask=True)\n",
    "        x = x + attention_output                                        # Residual connection\n",
    "        x = self.layer_norm2(x)                                         # Apply layer2 normalization\n",
    "        attention_output, attention_scores = self.multi_head_attention(x, encoder_output, encoder_output, mask=False)\n",
    "        x = x + attention_output                                        # Residual connection\n",
    "        x = self.layer_norm3(x)                                         # Apply layer3 normalization\n",
    "        x = x + self.feed_forward(x)                                    # Residual connection\n",
    "        return x, attention_scores                                      # Return the output and attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf4301",
   "metadata": {},
   "source": [
    "*What if we input encoder layers's input in every decoder layer or Each Encoder layer's Output in each Decoder layer's Input  ?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c7977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorrectly Implemented as we itertevely require the encoder_output\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layers = nn.ModuleList([Decoder_Layer(config) for _ in range(config.num_decoder_layers)])\n",
    "            \n",
    "    def forward(self, x, encoder_output):\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    num_decoder_layers = 2\n",
    "    embed_dim = 512\n",
    "    n_heads = 8\n",
    "    ff_dim = 2048\n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49886d4",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "*Let Now Define Transofmer Seq-2-Seq Architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0114d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.embedding_layer = SinusoidalEmbeddingLayer(config.vocab_size, config.embed_dim, config.max_length, config.device)\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([Decoder_Layer(config) for _ in range(config.num_layers)])\n",
    "        self.encoder_layers = nn.ModuleList([Encoder_Layer(config) for _ in range(config.num_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(config.embed_dim, config.embed_dim)        # Final linear layer\n",
    "    \n",
    "    def forward(self, source_input, target_input, Coupled=False):\n",
    "        src = self.embedding_layer(source_input)                                    # Shape: (batch, seq_len, embed_dim)\n",
    "        tgt = self.embedding_layer(target_input)                                    # Shape: (batch, seq_len, embed_dim)\n",
    "\n",
    "        if Coupled:\n",
    "            for encoder_layer, decoder_layer in zip(self.encoder_layers, self.decoder_layers):\n",
    "                src, attention_output1 = encoder_layer(src)                                                # Encoder Layer\n",
    "                tgt, attention_output2 = decoder_layer(tgt, src)   \n",
    "        \n",
    "        else:\n",
    "            for encoder_layer in self.encoder_layers:\n",
    "                src, attention_output = encoder_layer(src)\n",
    "\n",
    "            for decoder_layer in self.decoder_layers:\n",
    "                tgt, attention_output = decoder_layer(tgt, src)\n",
    "\n",
    "        output = self.fc_out(tgt)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
