{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "884e4ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d117017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18143ac1",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "*The Idea is to Augument the token embeddings with position-dependednt pattern of values.* \n",
    "##\n",
    "*If the Pattern is characterisitc for each position, then other layers could learn to incoperate positional information into their transformation.*\n",
    "##\n",
    "***In Other words, if each position has a unique encoding, the model can infer order and distance between tokens.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39118b",
   "metadata": {},
   "source": [
    "### nn.Embedding: Maps the Descrete Input Tokens into Dense Vectors (Embeddings) , a learnable look-up table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d8246",
   "metadata": {},
   "source": [
    "*What if we don't use Embeddings for learning, or transforming into more complex term, instead use things as it , could be use linear with it ?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19de641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of nn.Embedding\n",
    "Num_Unique_Tokens = 1000\n",
    "Size_Of_Vector_to_map = 64\n",
    "enmbedding_layer = nn.Embedding(num_embeddings=Num_Unique_Tokens, embedding_dim=Size_Of_Vector_to_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e0b3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "positional_indices = torch.arange(0, 10).unsqueeze(0)\n",
    "print(positional_indices.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa543577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 5, 7, 9]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_indices[:, 1::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafeebcf",
   "metadata": {},
   "source": [
    "*For Positional Informations, max_length defines how long the input sequences can be that the model will see.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d263ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Positional Information using indicies Information, though lacks relative positional information\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_length):\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layer_norm = nn.LayerNorm(embed_size, eps=1e-12)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        word_embedding = self.embedding(x)                                                  # Convert unique word tokens to word embeddings\n",
    "        \n",
    "        positional_indices = torch.arange(x.size(-2), device=x.device).unsqueeze(0)         # Creates positional inidices tensor                             Shape: (1, Seqlen)\n",
    "        positional_embeddings = self.position_embedding(positional_indices)                 # Convert positional indicies to positional embeddings          Shape: (1, Seqlen, embed_size)  \n",
    "        \n",
    "        x = word_embedding + positional_embeddings                                          # Adds word embedding to positional embedding\n",
    "        x = self.layer_norm(x)                                                              # Apply layer normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5448099",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usage With Config\n",
    "class Config:\n",
    "    vocab_size = 1000\n",
    "    embed_size = 64\n",
    "    max_length = 100    # Maximum length of the input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929bdc09",
   "metadata": {},
   "source": [
    "## Usage ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b377b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding positional Information using sinusoidal function\n",
    "class SinusoidalEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_length, device):\n",
    "        super(SinusoidalEmbeddingLayer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        self.register_buffer(\"positional_embedding\", self._get_positional_encoding(max_length, embed_size, device))\n",
    "        self.layer_norm = nn.LayerNorm(embed_size, eps=1e-12)\n",
    "    \n",
    "    def _get_positional_encoding(self, max_length, embed_size, device):\n",
    "        pe = torch.zeros(max_length, embed_size, device=device)                              # Create a tensor of zeros of size (max_length, embed_size)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)              # Create a tensor of size (max_length, 1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))    # Create a tensor of exp values of 0 to embed_size/2\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)                                          # Apply sin function to even indices, start=0 , step=2\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)                                          # Apply cos function to odd indices, start=1, step=2\n",
    "        pe = pe.unsqueeze(0)                                                                  # shape: (1, max_length, embed_size)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        word_embedding = self.embedding(x)                                                  # Convert unique word tokens to word embeddings\n",
    "        \n",
    "        positional_embeddings = self.positional_embedding[:, :x.size(-2), :].to(x.device)   # Get sinosudal indicies information as positional embeddings          Shape: (1, Seqlen, embed_size)\n",
    "        x = word_embedding + positional_embeddings                                          # Adds word embedding to positional embedding\n",
    "        x = self.layer_norm(x)                                                              # Apply layer normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88310358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    vocab_size = 1000\n",
    "    embed_size = 64\n",
    "    max_length = 100    # Maximum length of the input sequence\n",
    "    device = device     # Device to use (CPU or GPU) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69ea65",
   "metadata": {},
   "source": [
    "## Feed Forward Layer\n",
    "*It Process Each Embedding Sepeartly insteaad of processing whole as single vector, that's why it is also called **Position-Wise Feed Forward Layer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa7e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, ff_dim, droupout=0.5):    # ff_dim is usally higher that model_dim\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, model_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.droupout = nn.Dropout(droupout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.droupout(x)       # apply dropout to the output of the second linear layer to reduce overfitting\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26db5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    model_dim = 512\n",
    "    ff_dim = 2048   # usually 4 times model_dim\n",
    "    dropout = 0.1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e6dda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForward(\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (droupout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "feed_forward = FeedForward(Config.model_dim, Config.ff_dim, Config.dropout).to(device)\n",
    "print(feed_forward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
